#!/usr/bin/env python
# coding: utf-8

# In[156]:


import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.decomposition import PCA



df = pd.read_csv('/Users/sem/Documents/VScode-projects/Python stuffs/malware_detection/final_dataset.csv')

# saved target variable columns
target_label = df['HAS_MALWARE']

# separated feature labels
feature_labels = df.drop(['HAS_MALWARE', 'MALWARE_TYPE'], axis=1)

# determining best number of principal components 
pca = PCA().fit(feature_labels)
fig, ax = plt.subplots()
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xticks(np.arange(0, 400, step=30))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.axhline(y=0.95, color='r', linestyle='-')
ax.grid(axis='x')
plt.show()




#PCA with 45 components covers at least 95% variance. PCA with 70 components covers at least 99% variance
pca = PCA(n_components=45)

# PCA projection 
pca.fit(feature_labels)
reduced = pca.transform(feature_labels)

#train test ratio 70 30
x_train, x_test, y_train, y_test = train_test_split(feature_labels, target_label, test_size=0.25, random_state = 0)



# In[162]:


from sklearn.tree import DecisionTreeClassifier 
# Create Decision Tree classifer object
decision_tree_model = DecisionTreeClassifier(criterion="entropy", max_depth = 3,random_state = 0)

# Train Decision Tree Classifer
decision_tree_model = decision_tree_model.fit(x_train,y_train)

#Predict Output
y_pred = decision_tree_model.predict(x_test)
matrix = confusion_matrix(y_test, y_pred)

TN = matrix[0][0]
FP = matrix[0][1]
FN = matrix[1][0]
TP = matrix[1][1]

accuracy = accuracy_score(y_test, y_pred)
recall = TP/(TP+FN)
precision = TP/(TP+FP)
f1 = 2*((precision*recall)/(precision+recall))

print("DECISION TREE:")
print(matrix)
print("TN: ", TN)
print("FP: ", FP)
print("FN: ", FN)
print("TP: ", TP)

scores = cross_val_score(naive_bayes_model, x_train, y_train, scoring = 'accuracy', cv=5)
print("Cross-validation: ", scores.mean())

print("Accuracy Score: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1-Score: ", f1)

print("\n")


# In[164]:


from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier
naive_bayes_model = GaussianNB()

# Train the model using the training sets
naive_bayes_model.fit(x_train,y_train)

#Predict Output
y_pred = naive_bayes_model.predict(x_test)
matrix = confusion_matrix(y_test, y_pred)

TN = matrix[0][0]
FP = matrix[0][1]
FN = matrix[1][0]
TP = matrix[1][1]

accuracy = accuracy_score(y_test, y_pred)
recall = TP/(TP+FN)
precision = TP/(TP+FP)
f1 = 2*((precision*recall)/(precision+recall))

print("NAIVE BAYES:")
print(matrix)
print("TN: ", TN)
print("FP: ", FP)
print("FN: ", FN)
print("TP: ", TP)

scores = cross_val_score(naive_bayes_model, x_train, y_train, scoring = 'accuracy', cv=5)
print("Cross-validation: ", scores.mean())

print("Accuracy Score: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1-Score: ", f1)
print("\n")


# In[165]:


from sklearn.svm import SVC 

svm_model = SVC(kernel='linear')
svm_model.fit(x_train,y_train)
    
#Predict Output
y_pred = svm_model.predict(x_test)
matrix = confusion_matrix(y_test, y_pred)

TN = matrix[0][0]
FP = matrix[0][1]
FN = matrix[1][0]
TP = matrix[1][1]

accuracy = accuracy_score(y_test, y_pred)
recall = TP/(TP+FN)
precision = TP/(TP+FP)
f1 = 2*((precision*recall)/(precision+recall))

print("SVM:")
print(matrix)
print("TN: ", TN)
print("FP: ", FP)
print("FN: ", FN)
print("TP: ", TP)

scores = cross_val_score(svm_model, x_train, y_train, scoring = 'accuracy', cv=5)
print("Cross-validation: ", scores.mean())

print("Accuracy Score: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1-Score: ", f1)

print("\n")



          


# In[196]:


from sklearn.ensemble import VotingClassifier

voting_classifer_model = VotingClassifier(
                             estimators=[
                                 ('decision_tree', decision_tree_model), 
                                 ('naive_Bayes', naive_bayes_model), 
                                 ('svm', svm_model)],
                                 voting='hard')

voting_classifer_model.fit(x_train, y_train)
y_pred = voting_classifer_model.predict(x_test)


#Predict Output
matrix = confusion_matrix(y_test, y_pred)

TN = matrix[0][0]
FP = matrix[0][1]
FN = matrix[1][0]
TP = matrix[1][1]

accuracy = accuracy_score(y_test, y_pred)
recall = TP/(TP+FN)
precision = TP/(TP+FP)
f1 = 2*((precision*recall)/(precision+recall))

print("Voting Classifier: ")
print(matrix)
print("TN: ", TN)
print("FP: ", FP)
print("FN: ", FN)
print("TP: ", TP)

scores = cross_val_score(voting_classifer_model, x_train, y_train, scoring = 'accuracy', cv=5)
print("Cross-validation: ", scores.mean())

print("Accuracy Score: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1-Score: ", f1)

print("\n")


# In[197]:


from sklearn.ensemble import BaggingClassifier

# bagging classifier
bagging_ensemble = BaggingClassifier(
                          base_estimator = svm_model,
                          random_state = 0)

bagging_ensemble.fit(x_train, y_train)
y_pred = bagging_ensemble.predict(x_test)


#Predict Output
matrix = confusion_matrix(y_test, y_pred)

TN = matrix[0][0]
FP = matrix[0][1]
FN = matrix[1][0]
TP = matrix[1][1]

accuracy = accuracy_score(y_test, y_pred)
recall = TP/(TP+FN)
precision = TP/(TP+FP)
f1 = 2*((precision*recall)/(precision+recall))

print("Bagging Classifier: ")
print(matrix)
print("TN: ", TN)
print("FP: ", FP)
print("FN: ", FN)
print("TP: ", TP)

scores = cross_val_score(bagging_ensemble, x_train, y_train, scoring = 'accuracy', cv=5)
print("Cross-validation: ", scores.mean())

print("Accuracy Score: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1-Score: ", f1)

print("\n")


# In[198]:


from sklearn.ensemble import AdaBoostClassifier

booster_ensemble = AdaBoostClassifier(
                    base_estimator = decision_tree_model, 
                    n_estimators=70, 
                    random_state=0)
booster_ensemble.fit(x_train, y_train)
y_pred = booster_ensemble.predict(x_test)


#Predict Output
matrix = confusion_matrix(y_test, y_pred)

TN = matrix[0][0]
FP = matrix[0][1]
FN = matrix[1][0]
TP = matrix[1][1]

accuracy = accuracy_score(y_test, y_pred)
recall = TP/(TP+FN)
precision = TP/(TP+FP)
f1 = 2*((precision*recall)/(precision+recall))

print("AdaBoost Classifier: ")
print(matrix)
print("TN: ", TN)
print("FP: ", FP)
print("FN: ", FN)
print("TP: ", TP)

scores = cross_val_score(booster_ensemble, x_train, y_train, scoring = 'accuracy', cv=5)
print("Cross-validation: ", scores.mean())

print("Accuracy Score: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1-Score: ", f1)

print("\n")




# In[214]:


from sklearn.metrics import roc_curve, roc_auc_score

voting_y_pred = voting_classifer_model.predict(x_test)
pred_prob2 = bagging_ensemble.predict_proba(x_test)
pred_prob3 = booster_ensemble.predict_proba(x_test)

# roc curve for models
fpr1, tpr1, thresh1 = roc_curve(y_test, voting_y_pred, pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)
fpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob3[:,1], pos_label=1)


# roc curve for tpr = fpr 
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)

# matplotlib
import matplotlib.pyplot as plt
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Voting Ensemble Classifier, AUC='+str(round(roc_auc_score(y_test, voting_y_pred),2)))
plt.plot(fpr2, tpr2, linestyle='--',color='green', label='Bagging Classifier, AUC='+str(round(roc_auc_score(y_test, pred_prob2[:,1]),2)))
plt.plot(fpr3, tpr3, linestyle='--',color='yellow', label='AdaBoost Classifier, AUC='+str(round(roc_auc_score(y_test, pred_prob3[:,1]),2)))
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();


# In[ ]:





# In[ ]:




